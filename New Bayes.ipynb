{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This work is the result of my own tortures and talent of other author, whose ideas I borrowed (a bit). In order to be honest I include the link to his work in the end of the notebook.\n",
    "\n",
    "###### When we plagiarize, we are likewise creating in the image and participating in the completion of Creation\n",
    "\n",
    "                                                                             Jonathan Safran Foer, Everything Is Illuminated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Forest_Gump\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk import PorterStemmer\n",
    "import string\n",
    "from sklearn import naive_bayes\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us import the data and look at them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     v1                                                 v2 Unnamed: 2  \\\n",
       "0   ham  Go until jurong point, crazy.. Available only ...        NaN   \n",
       "1   ham                      Ok lar... Joking wif u oni...        NaN   \n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4  \n",
       "0        NaN        NaN  \n",
       "1        NaN        NaN  \n",
       "2        NaN        NaN  "
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#A-ha, 3 strange columns without names, which seem to contain only NaN\n",
    "data = pd.read_csv('Downloads\\smsspam.csv')\n",
    "data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, ' MK17 92H. 450Ppw 16\"', ' why to miss them', 'GE',\n",
       "       'U NO THECD ISV.IMPORTANT TOME 4 2MORO\\\\\"\"',\n",
       "       'i wil tolerat.bcs ur my someone..... But',\n",
       "       ' ILLSPEAK 2 U2MORO WEN IM NOT ASLEEP...\\\\\"\"',\n",
       "       'whoever is the KING\\\\\"!... Gud nyt\"', ' TX 4 FONIN HON',\n",
       "       ' \\\\\"OH No! COMPETITION\\\\\". Who knew', 'IеХL CALL U\\\\\"\"'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#These strange columns contain some info that can be useful further, so we cannot just drop them\n",
    "data['Unnamed: 3'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: v1, dtype: int64"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.v1.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As measure of classification accuracy I decided to use 3 classic metrics: precision, recall and accuracy. 'Ham' is predicted as positive class, 'spam' - as negative. Therefore, we are interested in lower false negative (in order not to classify important ham messages as spam).If the classifier is good, I expect to see high recall for both ham and spam, and also high precision for spam. Accuracy in this case will give us general picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For futher work with text we need to prepare messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining a function for text preparation\n",
    "\n",
    "def cleaning(cleantext):\n",
    "    #lowercasing\n",
    "    cleantext = [d.lower() for d in cleantext] \n",
    "    #removing punctuation and stopwords\n",
    "    cleantext = ''.join([i for i in cleantext if i not in string.punctuation])\n",
    "    cleantext = [i for i in cleantext.split() if i not in stopwords.words('english')]\n",
    "    #stemming\n",
    "    stem = PorterStemmer()\n",
    "    cleantext = [stem.stem(i) for i in cleantext]\n",
    "    return cleantext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying cleaning function\n",
    "data['v2'] = data['v2'].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>[go, jurong, point, crazi, avail, bugi, n, gre...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>[ok, lar, joke, wif, u, oni]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    v1                                                 v2 Unnamed: 2  \\\n",
       "0  ham  [go, jurong, point, crazi, avail, bugi, n, gre...        NaN   \n",
       "1  ham                       [ok, lar, joke, wif, u, oni]        NaN   \n",
       "\n",
       "  Unnamed: 3 Unnamed: 4 predicted  \n",
       "0        NaN        NaN       ham  \n",
       "1        NaN        NaN       ham  "
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check our clean data\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some problem occured when I tried to perform 5-fold validation in a usual manner, therefore, I decided to make 5-fold validaion \"by hands\". I got indices through KFold and extracted sets from dataset with 'clean' messages. For each set a vocabulary was created, words were count, then through functions probabilities were calculated. Them classification was performed. Chosen metrics (precision, recall, accuracy) were also calculated \"by hands\". At the end of this strange cross-validation the 'table' with al counted metrics is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [1115 1116 1117 ... 5569 5570 5571] TEST: [   0    1    2 ... 1112 1113 1114]\n",
      "TRAIN: [   0    1    2 ... 5569 5570 5571] TEST: [1115 1116 1117 ... 2227 2228 2229]\n",
      "TRAIN: [   0    1    2 ... 5569 5570 5571] TEST: [2230 2231 2232 ... 3341 3342 3343]\n",
      "TRAIN: [   0    1    2 ... 5569 5570 5571] TEST: [3344 3345 3346 ... 4455 4456 4457]\n",
      "TRAIN: [   0    1    2 ... 4455 4456 4457] TEST: [4458 4459 4460 ... 5569 5570 5571]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5)\n",
    "kf.split(data)\n",
    "for train_index, test_index in kf.split(data):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = data['v2'][train_index], data['v2'][test_index]\n",
    "    y_train, y_test = data['v1'][train_index], data['v1'][test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting set 1 and applying classification fucntion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set1\n",
    "X_train, X_test = data['v2'][1115:5572], data['v2'][0:1114]\n",
    "y_train, y_test =  data['v1'][1115:5572],  data['v1'][0:1114]\n",
    "train_data = pd.DataFrame([X_train, y_train])\n",
    "test_data =pd.DataFrame([X_test, y_test])\n",
    "train_data=train_data.transpose()\n",
    "test_data=test_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set(train_data['v2'].sum()))\n",
    "word_counts_per_sms = pd.DataFrame([\n",
    "    [row[1].count(word) for word in vocabulary]\n",
    "    for _, row in train_data.iterrows()], columns=vocabulary)\n",
    "train_data = pd.concat([train_data.reset_index(), word_counts_per_sms], axis=1).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pspam = train_data['v1'].value_counts()['spam'] / train_data.shape[0]\n",
    "Pham = train_data['v1'].value_counts()['ham'] / train_data.shape[0]\n",
    "Nspam = train_data.loc[train_data['v1'] == 'spam','v2'].apply(len).sum()\n",
    "Nham = train_data.loc[train_data['v1'] == 'ham','v2'].apply(len).sum()\n",
    "Nvoc = len(train_data.columns)\n",
    "#setting lambda\n",
    "l=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_word_is_spam(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'spam', word].sum() + l) / (Nspam + l*Nvoc)\n",
    "    else:\n",
    "        return 1\n",
    "      \n",
    "def p_word_is_ham(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'ham', word].sum() + l) / (Nham + l*Nvoc)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    p_spam_given_message = Pspam\n",
    "    p_ham_given_message = Pham\n",
    "    for word in message:\n",
    "        p_spam_given_message *= p_word_is_spam(word)\n",
    "        p_ham_given_message *= p_word_is_ham(word)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted']=test_data['v2'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "true=test_data['v1'].tolist()\n",
    "predicted=test_data['predicted'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive 1:  961\n",
      "True negative 1:  139\n",
      "False positive 1:  6\n",
      "False negative 1:  8\n"
     ]
    }
   ],
   "source": [
    "true_positive1=0\n",
    "true_negative1=0\n",
    "false_positive1=0\n",
    "false_negative1=0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    if true[i]=='spam' and predicted[i]=='spam':\n",
    "        true_negative1+=1\n",
    "    elif true[i]=='spam' and predicted[i]=='ham':\n",
    "        false_positive1+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='ham':\n",
    "        true_positive1+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='spam':\n",
    "        false_negative1+=1\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "print('True positive 1: ', true_positive1)\n",
    "print('True negative 1: ', true_negative1) \n",
    "print('False positive 1: ',false_positive1) \n",
    "print('False negative 1: ',false_negative1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for ham 1:  0.9937952430196484\n",
      "Recall for ham 1:  0.9917440660474717\n",
      "Precision for spam 1:  0.9455782312925171\n",
      "Recall for spam 1:  0.9586206896551724\n",
      "Accuracy 1:  0.9874326750448833\n"
     ]
    }
   ],
   "source": [
    "#Let us count the metrics: accuracy, precision and recall\n",
    "\n",
    "hamprecision1=true_positive1/(true_positive1 + false_positive1)\n",
    "hamrecall1=true_positive1/(true_positive1 + false_negative1)\n",
    "spamprecision1=true_negative1/(true_negative1 + false_negative1)\n",
    "spamrecall1=true_negative1/(true_negative1 + false_positive1)\n",
    "accuracy1=(true_positive1+true_negative1)/(true_positive1+true_negative1+false_positive1+false_negative1)\n",
    "\n",
    "print('Precision for ham 1: ',hamprecision1)\n",
    "print('Recall for ham 1: ',hamrecall1)\n",
    "print('Precision for spam 1: ',spamprecision1)\n",
    "print('Recall for spam 1: ',spamrecall1)\n",
    "print('Accuracy 1: ', accuracy1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting set 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set2\n",
    "X_train, X_test = data['v2'], data['v2'][1115:2229]\n",
    "y_train, y_test =  data['v1'],  data['v1'][1115:2229]\n",
    "train_data = pd.DataFrame([X_train, y_train])\n",
    "test_data =pd.DataFrame([X_test, y_test])\n",
    "train_data=train_data.transpose()\n",
    "train_data = train_data.drop(train_data.index[1115:2229])\n",
    "test_data=test_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set(train_data['v2'].sum()))\n",
    "word_counts_per_sms = pd.DataFrame([\n",
    "    [row[1].count(word) for word in vocabulary]\n",
    "    for _, row in train_data.iterrows()], columns=vocabulary)\n",
    "train_data = pd.concat([train_data.reset_index(), word_counts_per_sms], axis=1).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pspam = train_data['v1'].value_counts()['spam'] / train_data.shape[0]\n",
    "Pham = train_data['v1'].value_counts()['ham'] / train_data.shape[0]\n",
    "Nspam = train_data.loc[train_data['v1'] == 'spam','v2'].apply(len).sum()\n",
    "Nham = train_data.loc[train_data['v1'] == 'ham','v2'].apply(len).sum()\n",
    "Nvoc = len(train_data.columns)\n",
    "#setting lambda\n",
    "l=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_word_is_spam(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'spam', word].sum() + l) / (Nspam + l*Nvoc)\n",
    "    else:\n",
    "        return 1\n",
    "      \n",
    "def p_word_is_ham(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'ham', word].sum() + l) / (Nham + l*Nvoc)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    p_spam_given_message = Pspam\n",
    "    p_ham_given_message = Pham\n",
    "    for word in message:\n",
    "        p_spam_given_message *= p_word_is_spam(word)\n",
    "        p_ham_given_message *= p_word_is_ham(word)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted']=test_data['v2'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "true=test_data['v1'].tolist()\n",
    "predicted=test_data['predicted'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive 2:  961\n",
      "True negative 2:  139\n",
      "False positive 2:  6\n",
      "False negative 2:  8\n"
     ]
    }
   ],
   "source": [
    "true_positive2=0\n",
    "true_negative2=0\n",
    "false_positive2=0\n",
    "false_negative2=0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    if true[i]=='spam' and predicted[i]=='spam':\n",
    "        true_negative2+=1\n",
    "    elif true[i]=='spam' and predicted[i]=='ham':\n",
    "        false_positive2+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='ham':\n",
    "        true_positive2+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='spam':\n",
    "        false_negative2+=1\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "print('True positive 2: ', true_positive2)\n",
    "print('True negative 2: ', true_negative2) \n",
    "print('False positive 2: ',false_positive2) \n",
    "print('False negative 2: ',false_negative2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for ham 2:  0.9937952430196484\n",
      "Recall for ham 2:  0.9917440660474717\n",
      "Precision for spam 2:  0.9455782312925171\n",
      "Recall for spam 2:  0.9586206896551724\n",
      "Accuracy 2:  0.9874326750448833\n"
     ]
    }
   ],
   "source": [
    "#Let us count the metrics: accuracy, precision and recall\n",
    "\n",
    "hamprecision2=true_positive2/(true_positive2 + false_positive2)\n",
    "hamrecall2=true_positive2/(true_positive2 + false_negative2)\n",
    "spamprecision2=true_negative2/(true_negative2 + false_negative2)\n",
    "spamrecall2=true_negative2/(true_negative2 + false_positive2)\n",
    "accuracy2=(true_positive2+true_negative2)/(true_positive2+true_negative2+false_positive2+false_negative2)\n",
    "\n",
    "print('Precision for ham 2: ',hamprecision2)\n",
    "print('Recall for ham 2: ',hamrecall2)\n",
    "print('Precision for spam 2: ',spamprecision2)\n",
    "print('Recall for spam 2: ',spamrecall2)\n",
    "print('Accuracy 2: ', accuracy2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting 3 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set3\n",
    "X_train, X_test = data['v2'], data['v2'][2230:3344]\n",
    "y_train, y_test =  data['v1'],  data['v1'][2230:3344]\n",
    "train_data = pd.DataFrame([X_train, y_train])\n",
    "test_data =pd.DataFrame([X_test, y_test])\n",
    "train_data=train_data.transpose()\n",
    "train_data = train_data.drop(train_data.index[2230:3344])\n",
    "test_data=test_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set(train_data['v2'].sum()))\n",
    "word_counts_per_sms = pd.DataFrame([\n",
    "    [row[1].count(word) for word in vocabulary]\n",
    "    for _, row in train_data.iterrows()], columns=vocabulary)\n",
    "train_data = pd.concat([train_data.reset_index(), word_counts_per_sms], axis=1).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pspam = train_data['v1'].value_counts()['spam'] / train_data.shape[0]\n",
    "Pham = train_data['v1'].value_counts()['ham'] / train_data.shape[0]\n",
    "Nspam = train_data.loc[train_data['v1'] == 'spam','v2'].apply(len).sum()\n",
    "Nham = train_data.loc[train_data['v1'] == 'ham','v2'].apply(len).sum()\n",
    "Nvoc = len(train_data.columns)\n",
    "#setting lambda\n",
    "l=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_word_is_spam(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'spam', word].sum() + l) / (Nspam + l*Nvoc)\n",
    "    else:\n",
    "        return 1\n",
    "      \n",
    "def p_word_is_ham(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'ham', word].sum() + l) / (Nham + l*Nvoc)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    p_spam_given_message = Pspam\n",
    "    p_ham_given_message = Pham\n",
    "    for word in message:\n",
    "        p_spam_given_message *= p_word_is_spam(word)\n",
    "        p_ham_given_message *= p_word_is_ham(word)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted']=test_data['v2'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "true=test_data['v1'].tolist()\n",
    "predicted=test_data['predicted'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive 3:  961\n",
      "True negative 3:  139\n",
      "False positive 3:  6\n",
      "False negative 3:  8\n"
     ]
    }
   ],
   "source": [
    "true_positive3=0\n",
    "true_negative3=0\n",
    "false_positive3=0\n",
    "false_negative3=0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    if true[i]=='spam' and predicted[i]=='spam':\n",
    "        true_negative3+=1\n",
    "    elif true[i]=='spam' and predicted[i]=='ham':\n",
    "        false_positive3+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='ham':\n",
    "        true_positive3+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='spam':\n",
    "        false_negative3+=1\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "print('True positive 3: ', true_positive3)\n",
    "print('True negative 3: ', true_negative3) \n",
    "print('False positive 3: ',false_positive3) \n",
    "print('False negative 3: ',false_negative3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for ham 3:  0.9937952430196484\n",
      "Recall for ham 3:  0.9917440660474717\n",
      "Precision for spam 3:  0.9455782312925171\n",
      "Recall for spam 3:  0.9586206896551724\n",
      "Accuracy 3:  0.9874326750448833\n"
     ]
    }
   ],
   "source": [
    "#Let us count the metrics: accuracy, precision and recall\n",
    "\n",
    "hamprecision3=true_positive3/(true_positive3 + false_positive3)\n",
    "hamrecall3=true_positive3/(true_positive3 + false_negative3)\n",
    "spamprecision3=true_negative3/(true_negative3 + false_negative3)\n",
    "spamrecall3=true_negative3/(true_negative3 + false_positive3)\n",
    "accuracy3=(true_positive3+true_negative3)/(true_positive3+true_negative3+false_positive3+false_negative3)\n",
    "\n",
    "print('Precision for ham 3: ',hamprecision3)\n",
    "print('Recall for ham 3: ',hamrecall3)\n",
    "print('Precision for spam 3: ',spamprecision3)\n",
    "print('Recall for spam 3: ',spamrecall3)\n",
    "print('Accuracy 3: ', accuracy3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting 4 set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set4\n",
    "X_train, X_test = data['v2'], data['v2'][3344:4458]\n",
    "y_train, y_test =  data['v1'],  data['v1'][3344:4458]\n",
    "train_data = pd.DataFrame([X_train, y_train])\n",
    "test_data =pd.DataFrame([X_test, y_test])\n",
    "train_data=train_data.transpose()\n",
    "train_data = train_data.drop(train_data.index[3344:4458])\n",
    "test_data=test_data.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set(train_data['v2'].sum()))\n",
    "word_counts_per_sms = pd.DataFrame([\n",
    "    [row[1].count(word) for word in vocabulary]\n",
    "    for _, row in train_data.iterrows()], columns=vocabulary)\n",
    "train_data = pd.concat([train_data.reset_index(), word_counts_per_sms], axis=1).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pspam = train_data['v1'].value_counts()['spam'] / train_data.shape[0]\n",
    "Pham = train_data['v1'].value_counts()['ham'] / train_data.shape[0]\n",
    "Nspam = train_data.loc[train_data['v1'] == 'spam','v2'].apply(len).sum()\n",
    "Nham = train_data.loc[train_data['v1'] == 'ham','v2'].apply(len).sum()\n",
    "Nvoc = len(train_data.columns)\n",
    "#setting lambda\n",
    "l=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_word_is_spam(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'spam', word].sum() + l) / (Nspam + l*Nvoc)\n",
    "    else:\n",
    "        return 1\n",
    "      \n",
    "def p_word_is_ham(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'ham', word].sum() + l) / (Nham + l*Nvoc)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    p_spam_given_message = Pspam\n",
    "    p_ham_given_message = Pham\n",
    "    for word in message:\n",
    "        p_spam_given_message *= p_word_is_spam(word)\n",
    "        p_ham_given_message *= p_word_is_ham(word)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted']=test_data['v2'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "true=test_data['v1'].tolist()\n",
    "predicted=test_data['predicted'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive 4:  961\n",
      "True negative 4:  139\n",
      "False positive 4:  6\n",
      "False negative 4:  8\n"
     ]
    }
   ],
   "source": [
    "true_positive4=0\n",
    "true_negative4=0\n",
    "false_positive4=0\n",
    "false_negative4=0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    if true[i]=='spam' and predicted[i]=='spam':\n",
    "        true_negative4+=1\n",
    "    elif true[i]=='spam' and predicted[i]=='ham':\n",
    "        false_positive4+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='ham':\n",
    "        true_positive4+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='spam':\n",
    "        false_negative4+=1\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "print('True positive 4: ', true_positive4)\n",
    "print('True negative 4: ', true_negative4) \n",
    "print('False positive 4: ',false_positive4) \n",
    "print('False negative 4: ',false_negative4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for ham 4:  0.9937952430196484\n",
      "Recall for ham 4:  0.9917440660474717\n",
      "Precision for spam 4:  0.9455782312925171\n",
      "Recall for spam 4:  0.9586206896551724\n",
      "Accuracy 4:  0.9874326750448833\n"
     ]
    }
   ],
   "source": [
    "#Let us count the metrics: accuracy, precision and recall\n",
    "\n",
    "hamprecision4=true_positive4/(true_positive4 + false_positive4)\n",
    "hamrecall4=true_positive4/(true_positive4 + false_negative4)\n",
    "spamprecision4=true_negative4/(true_negative4 + false_negative4)\n",
    "spamrecall4=true_negative4/(true_negative4 + false_positive4)\n",
    "accuracy4=(true_positive4+true_negative4)/(true_positive4+true_negative4+false_positive4+false_negative4)\n",
    "\n",
    "print('Precision for ham 4: ',hamprecision4)\n",
    "print('Recall for ham 4: ',hamrecall4)\n",
    "print('Precision for spam 4: ',spamprecision4)\n",
    "print('Recall for spam 4: ',spamrecall4)\n",
    "print('Accuracy 4: ', accuracy4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extracting the last (5th) set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set5\n",
    "X_train, X_test = data['v2'], data['v2'][4458:5572]\n",
    "y_train, y_test =  data['v1'],  data['v1'][4458:5572]\n",
    "train_data = pd.DataFrame([X_train, y_train])\n",
    "test_data =pd.DataFrame([X_test, y_test])\n",
    "train_data=train_data.transpose()\n",
    "train_data = train_data.drop(train_data.index[4458:5572])\n",
    "test_data=test_data.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = list(set(train_data['v2'].sum()))\n",
    "word_counts_per_sms = pd.DataFrame([\n",
    "    [row[1].count(word) for word in vocabulary]\n",
    "    for _, row in train_data.iterrows()], columns=vocabulary)\n",
    "train_data = pd.concat([train_data.reset_index(), word_counts_per_sms], axis=1).iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pspam = train_data['v1'].value_counts()['spam'] / train_data.shape[0]\n",
    "Pham = train_data['v1'].value_counts()['ham'] / train_data.shape[0]\n",
    "Nspam = train_data.loc[train_data['v1'] == 'spam','v2'].apply(len).sum()\n",
    "Nham = train_data.loc[train_data['v1'] == 'ham','v2'].apply(len).sum()\n",
    "Nvoc = len(train_data.columns)\n",
    "#setting lambda\n",
    "l=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_word_is_spam(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'spam', word].sum() + l) / (Nspam + l*Nvoc)\n",
    "    else:\n",
    "        return 1\n",
    "      \n",
    "def p_word_is_ham(word):\n",
    "    if word in train_data.columns:\n",
    "        return (train_data.loc[train_data['v1'] == 'ham', word].sum() + l) / (Nham + l*Nvoc)\n",
    "    else:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(message):\n",
    "    p_spam_given_message = Pspam\n",
    "    p_ham_given_message = Pham\n",
    "    for word in message:\n",
    "        p_spam_given_message *= p_word_is_spam(word)\n",
    "        p_ham_given_message *= p_word_is_ham(word)\n",
    "    if p_ham_given_message > p_spam_given_message:\n",
    "        return 'ham'\n",
    "    elif p_ham_given_message < p_spam_given_message:\n",
    "        return 'spam'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data['predicted']=test_data['v2'].apply(classify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "true=test_data['v1'].tolist()\n",
    "predicted=test_data['predicted'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive 5:  961\n",
      "True negative 5:  139\n",
      "False positive 5:  6\n",
      "False negative 5:  8\n"
     ]
    }
   ],
   "source": [
    "true_positive5=0\n",
    "true_negative5=0\n",
    "false_positive5=0\n",
    "false_negative5=0\n",
    "\n",
    "for i in range(len(true)):\n",
    "    if true[i]=='spam' and predicted[i]=='spam':\n",
    "        true_negative5+=1\n",
    "    elif true[i]=='spam' and predicted[i]=='ham':\n",
    "        false_positive5+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='ham':\n",
    "        true_positive5+=1\n",
    "    elif true[i]=='ham' and predicted[i]=='spam':\n",
    "        false_negative5+=1\n",
    "    else:\n",
    "        print('Error')\n",
    "\n",
    "print('True positive 5: ', true_positive5)\n",
    "print('True negative 5: ', true_negative5) \n",
    "print('False positive 5: ',false_positive5) \n",
    "print('False negative 5: ',false_negative5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision for ham 5:  0.9937952430196484\n",
      "Recall for ham 5:  0.9917440660474717\n",
      "Precision for spam 5:  0.9455782312925171\n",
      "Recall for spam 5:  0.9586206896551724\n",
      "Accuracy 5:  0.9874326750448833\n"
     ]
    }
   ],
   "source": [
    "#Let us count the metrics: accuracy, precision and recall\n",
    "\n",
    "hamprecision5=true_positive5/(true_positive5 + false_positive5)\n",
    "hamrecall5=true_positive5/(true_positive5 + false_negative5)\n",
    "spamprecision5=true_negative5/(true_negative5 + false_negative5)\n",
    "spamrecall5=true_negative5/(true_negative5 + false_positive5)\n",
    "accuracy5=(true_positive5+true_negative5)/(true_positive5+true_negative5+false_positive5+false_negative5)\n",
    "\n",
    "print('Precision for ham 5: ',hamprecision5)\n",
    "print('Recall for ham 5: ',hamrecall5)\n",
    "print('Precision for spam 5: ',spamprecision5)\n",
    "print('Recall for spam 5: ',spamrecall5)\n",
    "print('Accuracy 5: ', accuracy5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Set №  Ham precisions  Spam precisions  Ham recalls  Spam recalls  Accuracy\n",
      "0     1        0.993795         0.945578     0.991744      0.958621  0.987433\n",
      "1     2        0.993795         0.945578     0.991744      0.958621  0.987433\n",
      "2     3        0.993795         0.945578     0.991744      0.958621  0.987433\n",
      "3     4        0.993795         0.945578     0.991744      0.958621  0.987433\n",
      "4     5        0.993795         0.945578     0.991744      0.958621  0.987433\n"
     ]
    }
   ],
   "source": [
    "metric={'Set №':['1', '2', '3', '4', '5'],\n",
    "        'Ham precisions': [hamprecision1,hamprecision2,hamprecision2,hamprecision4,hamprecision5],\n",
    "        'Spam precisions': [spamprecision1,spamprecision2,spamprecision2,spamprecision4,spamprecision5],\n",
    "        'Ham recalls': [hamrecall1,hamrecall2,hamrecall3,hamrecall4,hamrecall5],\n",
    "        'Spam recalls': [spamrecall1,spamrecall2,spamrecall3,spamrecall4,spamrecall5],\n",
    "        'Accuracy':[accuracy1,accuracy2,accuracy3,accuracy4,accuracy5] }\n",
    "metrics=pd.DataFrame(metric, columns = ['Set №', 'Ham precisions','Spam precisions', 'Ham recalls','Spam recalls','Accuracy'])\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As we can see, our target measures are quite high. For example, 95% of actual spam messages are classified as spam, and 99% of ham messages are classified correctly. Classification accuracy is equal to almost 99%, which is unexpectedly high for by hand coded classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now it is time to use sklearn naive_bayes and see how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vector = CountVectorizer()\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>v1</th>\n",
       "      <th>v2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5567</th>\n",
       "      <td>spam</td>\n",
       "      <td>This is the 2nd time we have tried 2 contact u...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5568</th>\n",
       "      <td>ham</td>\n",
       "      <td>Will М_ b going to esplanade fr home?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5569</th>\n",
       "      <td>ham</td>\n",
       "      <td>Pity, * was in mood for that. So...any other s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5570</th>\n",
       "      <td>ham</td>\n",
       "      <td>The guy did some bitching but I acted like i'd...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5571</th>\n",
       "      <td>ham</td>\n",
       "      <td>Rofl. Its true to its name</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5572 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        v1                                                 v2\n",
       "0      ham  Go until jurong point, crazy.. Available only ...\n",
       "1      ham                      Ok lar... Joking wif u oni...\n",
       "2     spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3      ham  U dun say so early hor... U c already then say...\n",
       "4      ham  Nah I don't think he goes to usf, he lives aro...\n",
       "...    ...                                                ...\n",
       "5567  spam  This is the 2nd time we have tried 2 contact u...\n",
       "5568   ham              Will М_ b going to esplanade fr home?\n",
       "5569   ham  Pity, * was in mood for that. So...any other s...\n",
       "5570   ham  The guy did some bitching but I acted like i'd...\n",
       "5571   ham                         Rofl. Its true to its name\n",
       "\n",
       "[5572 rows x 2 columns]"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdata=pd.read_csv('Downloads\\smsspam.csv')\n",
    "newdata=newdata.drop(columns=['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'])\n",
    "newdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newdata['v2'],newdata['v1'],test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow',CountVectorizer(analyzer='word')), # converts strings to integer counts\n",
    "    ('tfidf',TfidfTransformer()), # converts integer counts to weighted TF-IDF scores\n",
    "    ('classifier',MultinomialNB()) # train on TF-IDF vectors with Naive Bayes classifier\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('bow',\n",
       "                 CountVectorizer(analyzer='word', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.int64'>, encoding='utf-8',\n",
       "                                 input='content', lowercase=True, max_df=1.0,\n",
       "                                 max_features=None, min_df=1,\n",
       "                                 ngram_range=(1, 1), preprocessor=None,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                                 tokenizer=None, vocabulary=None)),\n",
       "                ('tfidf',\n",
       "                 TfidfTransformer(norm='l2', smooth_idf=True,\n",
       "                                  sublinear_tf=False, use_idf=True)),\n",
       "                ('classifier',\n",
       "                 MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 315,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.95      1.00      0.97       960\n",
      "        spam       1.00      0.67      0.80       155\n",
      "\n",
      "    accuracy                           0.95      1115\n",
      "   macro avg       0.97      0.84      0.89      1115\n",
      "weighted avg       0.96      0.95      0.95      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For sklearn naive_bayes results are a bit worse in terms of accuracy - 95% vs 99% got by 'hand-coded' classifier. 100% of ham messages are classified correctly, however recall for spam is much lower - only 67% of spam messages are classified as spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General conclusion\n",
    "\n",
    "#### Coded 'by hand' classifier performs ham/spam classification better than inbuilt sklearn Multinomial NB classifier. All calculated measures of accuracy are higher for hand-coded NB (and this is a great surprise for me :) ). Accuracy for sklern NB is worse by 4%, and spam recall is lower by almost 30%. All in all, even though the 5-fold validation was made in awful manner, the hand-coded NB classifier shows better results. Hooray!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to Pavel Horbonos's work: https://towardsdatascience.com/how-to-build-and-apply-naive-bayes-classification-for-spam-filtering-2b8d3308501"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
